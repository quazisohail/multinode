ubuntu 16 hvm
alltraffic


namenode> ssh-keygen
namenode> cat id_rsa.pub >> ~/.ssh/authorized_keys
datanode2> cat id_rsa.pub >> ~/.ssh/authorized_keys
datanode3> cat id_rsa.pub >> ~/.ssh/authorized_keys
datanode1> cat id_rsa.pub >> ~/.ssh/authorized_keys

this is for all nodes 

sudo apt-get update && sudo apt-get -y dist-upgrade
sudo apt-get -y install openjdk-8-jdk-headless

mkdir server
cd server
wget -c https://downloads.apache.org/hadoop/common/hadoop-2.10.1/hadoop-2.10.1.tar.gz
tar xvzf hadoop-2.10.1.tar.gz

/server/hadoop-2.7.3/etc/hadoop/hadoop-env.sh
replace this line  export JAVA_HOME=${JAVA_HOME}
with this line     export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

/server/hadoop-2.7.3/etc/hadoop/core-site.xml 
paste
<property>
    <name>fs.defaultFS</name>
    <value><nnode>:9000</value>
  </property>

sudo mkdir -p /usr/local/hadoop/hdfs/data
sudo chown -R ubuntu:ubuntu /usr/local/hadoop/hdfs/data


now create 4 copies of instances

for name node
/.ssh/config
#replace <nnode> with EC2 Public DNS for NameNode)
Host nnode
  HostName <nnode>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

Host dnode1
  HostName <dnode1>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

Host dnode2
  HostName <dnode2>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa

Host dnode3
  HostName <dnode3>
  User ubuntu
  IdentityFile ~/.ssh/id_rsa
  
  to verify
  ssh nnode
  ssh dnode1
  ssh dnode2
  ssh dnode3
  
  namenode hdfs
  server/hadoop-2.7.3/etc/hadoop/hdfs-site.xml

<property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hdfs/data</value>
  </property>
  
  Namenode: Setup MapReduce
  /server/hadoop-2.7.3/etc/hadoop/mapred-site.xml
  
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value><nnode>:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  
  Namenode: Setup YARN Properties
  /server/hadoop-2.7.3/etc/hadoop/yarn-site.xml
  
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value><nnode></value>
  </property>
  
  Namenode: Setup Master and Slaves
  
  /server/hadoop-2.7.3/etc/hadoop/masters
  replace <nnode> with the NameNode’s public DNS
  /server/hadoop-2.7.3/etc/hadoop/slaves
  replace each of <dnode1,dnode2,dnode3>, etc with the appropriate DateNode’s public DNS
  
  Configuring Data Nodes
  
  /server/hadoop-2.7.3/etc/hadoop/hdfs-site.xml
  
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hdfs/data</value>
  </property>
  
  Starting the Hadoop Cluster
  
  namenode> cd ~/server
namenode> ./hadoop-2.7.3/bin/hdfs namenode -format

namenode> ./hadoop-2.7.3/sbin/start-dfs.sh
namenode> ./hadoop-2.7.3/sbin/start-yarn.sh
namenode> ./hadoop-2.7.3/sbin/mr-jobhistory-daemon.sh start historyserver

Check the Web UI
<nnode>:50070 
